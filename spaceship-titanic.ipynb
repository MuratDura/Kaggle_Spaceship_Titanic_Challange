{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Required libraries for linear algebra and data processing\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# For displaying outputs in IPython environments\nfrom IPython.display import display, HTML\n\n# This block is used to list all data files in the \"../input/\" directory in Kaggle environments\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef create_scrollable_table(df, table_id, title):\n    \"\"\"\n    Create a scrollable HTML table from a pandas DataFrame.\n    \n    Parameters:\n    - df (pandas.DataFrame): The DataFrame to be converted to an HTML table.\n    - table_id (str): HTML ID attribute for the div containing the table. This is used for potential CSS or JS targeting.\n    - title (str): Title to be displayed above the table.\n    \n    Returns:\n    - str: An HTML string that represents a scrollable table.\n    \"\"\"\n    \n    # Start with the table's title\n    html = f'<h3>{title}</h3>'\n    \n    # Create a div for the table with a scrollable feature (200px fixed height)\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    \n    # Convert the DataFrame to an HTML table and append to the string\n    html += df.to_html()\n    \n    # Close the div\n    html += '</div>'\n    \n    return html\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-15T09:54:20.470155Z","iopub.execute_input":"2023-10-15T09:54:20.470563Z","iopub.status.idle":"2023-10-15T09:54:20.480291Z","shell.execute_reply.started":"2023-10-15T09:54:20.470534Z","shell.execute_reply":"2023-10-15T09:54:20.479387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the \"train.csv\" file from the \"spaceship-titanic\" directory and storing it in a pandas DataFrame named 'df'\ndf = pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.492093Z","iopub.execute_input":"2023-10-15T09:54:20.492497Z","iopub.status.idle":"2023-10-15T09:54:20.524508Z","shell.execute_reply.started":"2023-10-15T09:54:20.492456Z","shell.execute_reply":"2023-10-15T09:54:20.523379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the dimensions (number of rows and columns) of the 'df' DataFrame\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.533984Z","iopub.execute_input":"2023-10-15T09:54:20.534635Z","iopub.status.idle":"2023-10-15T09:54:20.541436Z","shell.execute_reply.started":"2023-10-15T09:54:20.534601Z","shell.execute_reply":"2023-10-15T09:54:20.540704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the first 5 rows of the 'df' DataFrame for a quick overview\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.597991Z","iopub.execute_input":"2023-10-15T09:54:20.598965Z","iopub.status.idle":"2023-10-15T09:54:20.618480Z","shell.execute_reply.started":"2023-10-15T09:54:20.598925Z","shell.execute_reply":"2023-10-15T09:54:20.617490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(df):\n    # Split 'PassengerId' column values by '_' and create a new 'Group_no' column\n    split_array = np.array([item.split(\"_\") for item in df[\"PassengerId\"]])\n    df[\"Group_no\"] = split_array[:, 0].astype(int)\n\n    # Drop unnecessary columns 'Name' and 'PassengerId'\n    df = df.drop('Name', axis=1)\n    df = df.drop(\"PassengerId\", axis=1)\n\n    # Calculate 'Spending' and the percentage of each activity in the total spending\n    df['Spending'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n    df[\"PercentageRoomService\"] = (df[\"RoomService\"] / df[\"Spending\"]) * 100\n    df[\"PercentageFoodCourt\"] = (df[\"FoodCourt\"] / df[\"Spending\"]) * 100\n    df[\"PercentageShoppingMall\"] = (df[\"ShoppingMall\"] / df[\"Spending\"]) * 100\n    df[\"PercentageSpa\"] = (df[\"Spa\"] / df[\"Spending\"]) * 100\n    df[\"PercentageVRDeck\"] = (df[\"VRDeck\"] / df[\"Spending\"]) * 100\n\n    # Create binary columns based on 'HomePlanet' and 'CryoSleep' conditions\n    df[\"Earth_Cryo\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"CryoSleep\"] == \"True\")).astype(int)\n    df[\"Earth_Cryo2\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"CryoSleep\"] == \"False\")).astype(int)\n    df[\"Europa_Cryro\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"CryoSleep\"] == \"True\")).astype(int)\n    df[\"Europa_Cryo2\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"CryoSleep\"] == \"False\")).astype(int)\n    df[\"Mars_Cryo\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"CryoSleep\"] == \"True\")).astype(int)\n    df[\"Mars_Cryo2\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"CryoSleep\"] == \"False\")).astype(int)\n\n    # Create binary columns based on 'HomePlanet' and 'Destination' conditions\n    df[\"Pair1\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"] == \"TRAPPIST-1e\")).astype(int)\n    df[\"Pair2\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"] == \"55 Cancri e\")).astype(int)\n    df[\"Pair3\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"] == \"PSO J318.5-22\")).astype(int)\n    df[\"Pair4\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"] == \"TRAPPIST-1e\")).astype(int)\n    df[\"Pair5\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"] == \"55 Cancri e\")).astype(int)\n    df[\"Pair6\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"] == \"PSO J318.5-22\")).astype(int)\n    df[\"Pair7\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"] == \"TRAPPIST-1e\")).astype(int)\n    df[\"Pair8\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"] == \"55 Cancri e\")).astype(int)\n    df[\"Pair9\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"] == \"PSO J318.5-22\")).astype(int)\n\n    # Convert 'Cabin' column to string and create 'side' and 'deck' columns based on its values\n    df[\"Cabin\"] = df[\"Cabin\"].astype(str)\n    df[\"side\"] = np.array([item[-1] for item in df[\"Cabin\"]], dtype=object)\n    df[\"deck\"] = np.array([item[0] for item in df[\"Cabin\"]], dtype=object)\n    df = df.drop(\"Cabin\", axis=1)\n\n    # Calculate square roots of certain columns\n    df[\"sq_RoomService\"] = np.sqrt(df[\"RoomService\"])\n    df[\"sq_Spa\"] = np.sqrt(df[\"Spa\"])\n    df[\"sq_VRDeck\"] = np.sqrt(df[\"VRDeck\"])\n    df[\"sq_Spending\"] = np.sqrt(df[\"Spending\"])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.621277Z","iopub.execute_input":"2023-10-15T09:54:20.622169Z","iopub.status.idle":"2023-10-15T09:54:20.640566Z","shell.execute_reply.started":"2023-10-15T09:54:20.622098Z","shell.execute_reply":"2023-10-15T09:54:20.639377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the first 15 rows of the 'df' DataFrame for a detailed overview\ndf.head(15)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.642658Z","iopub.execute_input":"2023-10-15T09:54:20.642991Z","iopub.status.idle":"2023-10-15T09:54:20.674887Z","shell.execute_reply.started":"2023-10-15T09:54:20.642964Z","shell.execute_reply":"2023-10-15T09:54:20.673538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting only the numerical features from the 'df' DataFrame\nnumerical_features = df.select_dtypes(include=[np.number])\n\n# Computing summary statistics for the selected numerical features\nsummary_stats = numerical_features.describe().T\n\n# Using the create_scrollable_table function to generate a scrollable HTML table for the summary statistics\nhtml_numerical = create_scrollable_table(summary_stats, \"numerical_features\", \"Summary statistics for numerical features\")\n\n# Displaying the generated HTML table in the notebook\ndisplay(HTML(html_numerical))","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.677146Z","iopub.execute_input":"2023-10-15T09:54:20.677466Z","iopub.status.idle":"2023-10-15T09:54:20.706429Z","shell.execute_reply.started":"2023-10-15T09:54:20.677440Z","shell.execute_reply":"2023-10-15T09:54:20.705369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting only the categorical features (including boolean) from the 'df' DataFrame\ncategorical_features = df.select_dtypes(include=[object, np.bool_])\n\n# Computing summary statistics for the selected categorical features\nsummary_stats = categorical_features.describe().T\n\n# Using the create_scrollable_table function to generate a scrollable HTML table for the summary statistics of categorical features\nhtml_categorical = create_scrollable_table(summary_stats, \"categorical_features\", \"Summary statistics for categorical features\")\n\n# Displaying the generated HTML table in the notebook\ndisplay(HTML(html_categorical))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.777688Z","iopub.execute_input":"2023-10-15T09:54:20.778086Z","iopub.status.idle":"2023-10-15T09:54:20.816468Z","shell.execute_reply.started":"2023-10-15T09:54:20.778055Z","shell.execute_reply":"2023-10-15T09:54:20.815728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the number of missing values for each column in the 'df' DataFrame\nnull_values = df.isnull().sum()\n\n# Using the create_scrollable_table function to generate a scrollable HTML table for the count of missing values\nhtml_null_values = create_scrollable_table(null_values.to_frame(), 'null_values', 'Null values in the dataset')\n\n# Calculating the percentage of missing values for each column in relation to the total number of rows\nmissing_percentage = (df.isnull().sum() / len(df)) / 100\n\n# Using the create_scrollable_table function to generate a scrollable HTML table for the percentage of missing values\nhtml_missing_percentage = create_scrollable_table(missing_percentage.to_frame(), 'missing_percentage', 'Percentage of missing values in the dataset')\n\n# Combining and displaying both HTML tables in the notebook\ndisplay(HTML(html_null_values + html_missing_percentage))","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.817596Z","iopub.execute_input":"2023-10-15T09:54:20.817881Z","iopub.status.idle":"2023-10-15T09:54:20.837508Z","shell.execute_reply.started":"2023-10-15T09:54:20.817858Z","shell.execute_reply":"2023-10-15T09:54:20.835847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the preprocessing function to the 'df' DataFrame and storing the result in 'X'\nX = preprocessing(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.873298Z","iopub.execute_input":"2023-10-15T09:54:20.873651Z","iopub.status.idle":"2023-10-15T09:54:20.943281Z","shell.execute_reply.started":"2023-10-15T09:54:20.873625Z","shell.execute_reply":"2023-10-15T09:54:20.942376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary modules and functions for data preprocessing and transformation\n\n# FunctionTransformer allows you to create a transformer from any callable Python object\nfrom sklearn.preprocessing import FunctionTransformer\n\n# Pipeline and make_pipeline help in sequentially applying a list of transforms and a final estimator\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# SimpleImputer and KNNImputer are used for filling missing values using various strategies\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n# StandardScaler standardizes features by removing the mean and scaling to unit variance\n# OneHotEncoder is used for encoding categorical variables as a one-hot numeric array\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# ColumnTransformer is for applying transformers to columns of an array or pandas DataFrame\nfrom sklearn.compose import ColumnTransformer\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.945587Z","iopub.execute_input":"2023-10-15T09:54:20.945974Z","iopub.status.idle":"2023-10-15T09:54:20.951255Z","shell.execute_reply.started":"2023-10-15T09:54:20.945937Z","shell.execute_reply":"2023-10-15T09:54:20.950193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_array = np.array([item.split(\"_\")for item in df[\"PassengerId\"]])\ndf[\"Group_no\"] = split_array[:,0].astype(int)\n    \ndf = df.drop('Name',axis=1)\ndf = df.drop(\"PassengerId\",axis=1)\n    \ndf['Spending'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\ndf[\"PercentageRoomService\"] = (df[\"RoomService\"]/df[\"Spending\"])*100\ndf[\"PercentageFoodCourt\"] = (df[\"FoodCourt\"]/df[\"Spending\"])*100\ndf[\"PercentageShoppingMall\"] = (df[\"ShoppingMall\"]/df[\"Spending\"])*100\ndf[\"PercentageSpa\"] = (df[\"Spa\"]/df[\"Spending\"])*100\ndf[\"PercentageVRDeck\"] = (df[\"VRDeck\"]/df[\"Spending\"])*100\n\n\ndf[\"Earth_Cryo\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"CryoSleep\"]== \"True\")).astype(int)\ndf[\"Earth_Cryo2\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"CryoSleep\"]== \"False\")).astype(int)\ndf[\"Europa_Cryro\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"CryoSleep\"]== \"True\")).astype(int)\ndf[\"Europa_Cryo2\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"CryoSleep\"]== \"False\")).astype(int)\ndf[\"Mars_Cryo\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"CryoSleep\"]== \"True\")).astype(int)\ndf[\"Mars_Cryo2\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"CryoSleep\"]== \"False\")).astype(int)\n\ndf[\"Pair1\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"]== \"TRAPPIST-1e\")).astype(int)\ndf[\"Pair2\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"]== \"55 Cancri e\")).astype(int)\ndf[\"Pair3\"] = ((df[\"HomePlanet\"] == \"Earth\") & (df[\"Destination\"]== \"PSO J318.5-22\")).astype(int)\ndf[\"Pair4\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"]== \"TRAPPIST-1e\")).astype(int)\ndf[\"Pair5\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"]== \"55 Cancri e\")).astype(int)\ndf[\"Pair6\"] = ((df[\"HomePlanet\"] == \"Europa\") & (df[\"Destination\"]== \"PSO J318.5-22\")).astype(int)\ndf[\"Pair7\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"]== \"TRAPPIST-1e\")).astype(int)\ndf[\"Pair8\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"]== \"55 Cancri e\")).astype(int)\ndf[\"Pair9\"] = ((df[\"HomePlanet\"] == \"Mars\") & (df[\"Destination\"]== \"PSO J318.5-22\")).astype(int)\ndf[\"Cabin\"] = df[\"Cabin\"].astype(str)\ndf[\"side\"] = np.array([item[-1]for item in df[\"Cabin\"]],dtype=object)\ndf[\"deck\"] = np.array([item[0]for item in df[\"Cabin\"]],dtype=object)\ndf = df.drop(\"Cabin\",axis=1)\ndf[\"sq_RoomService\"] = np.sqrt(df[\"RoomService\"])\ndf[\"sq_Spa\"] = np.sqrt(df[\"Spa\"])\ndf[\"sq_VRDeck\"] = np.sqrt(df[\"VRDeck\"])\ndf[\"sq_Spending\"] = np.sqrt(df[\"Spending\"])","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:20.952912Z","iopub.execute_input":"2023-10-15T09:54:20.953332Z","iopub.status.idle":"2023-10-15T09:54:21.035498Z","shell.execute_reply.started":"2023-10-15T09:54:20.953296Z","shell.execute_reply":"2023-10-15T09:54:21.034227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a transformation pipeline for numerical features:\n# 1. Imputing missing values with the median of the column.\n# 2. Scaling the values to have zero mean and unit variance using StandardScaler.\n\nnumerical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),  # Impute missing values using the median of the column\n    (\"scaler\", StandardScaler())  # Scale the data\n])\n\n# Creating a transformation pipeline for categorical features:\n# 1. Imputing missing values with the most frequent value in the column.\n# 2. One-hot encoding the categorical variables.\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values using the most frequent value\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # One-hot encode the data\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.037988Z","iopub.execute_input":"2023-10-15T09:54:21.038393Z","iopub.status.idle":"2023-10-15T09:54:21.043875Z","shell.execute_reply.started":"2023-10-15T09:54:21.038357Z","shell.execute_reply":"2023-10-15T09:54:21.042846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying categorical columns in the 'df' DataFrame.\n# We select columns that have data type 'object' or 'category'.\ncategorical_columns = df.select_dtypes(include=[\"object\", \"category\"]).columns\n\n# Identifying numerical columns in the 'df' DataFrame.\n# We select columns that have data type 'int64' or 'float64'.\nnumerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.045445Z","iopub.execute_input":"2023-10-15T09:54:21.045737Z","iopub.status.idle":"2023-10-15T09:54:21.060598Z","shell.execute_reply.started":"2023-10-15T09:54:21.045712Z","shell.execute_reply":"2023-10-15T09:54:21.059669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_columns","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.062504Z","iopub.execute_input":"2023-10-15T09:54:21.062887Z","iopub.status.idle":"2023-10-15T09:54:21.070810Z","shell.execute_reply.started":"2023-10-15T09:54:21.062850Z","shell.execute_reply":"2023-10-15T09:54:21.070001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.071933Z","iopub.execute_input":"2023-10-15T09:54:21.072238Z","iopub.status.idle":"2023-10-15T09:54:21.082420Z","shell.execute_reply.started":"2023-10-15T09:54:21.072198Z","shell.execute_reply":"2023-10-15T09:54:21.081637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting all values in the categorical columns of 'df' DataFrame to string type\ndf[categorical_columns] = df[categorical_columns].astype(str)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.084357Z","iopub.execute_input":"2023-10-15T09:54:21.084611Z","iopub.status.idle":"2023-10-15T09:54:21.101691Z","shell.execute_reply.started":"2023-10-15T09:54:21.084587Z","shell.execute_reply":"2023-10-15T09:54:21.100668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up the ColumnTransformer that will apply the specified preprocessing steps to the specified columns.\n# The 'num' transformer applies the 'numerical_transformer' to the 'numerical_columns'.\n# The 'cat' transformer applies the 'categorical_transformer' to the 'categorical_columns'.\n# Any other columns not specified will be 'passed through' without any changes (due to 'remainder=\"passthrough\"').\npreprocessor = ColumnTransformer(transformers=[\n    (\"num\", numerical_transformer, numerical_columns),\n    (\"cat\", categorical_transformer, categorical_columns)\n], remainder=\"passthrough\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.103077Z","iopub.execute_input":"2023-10-15T09:54:21.103528Z","iopub.status.idle":"2023-10-15T09:54:21.108076Z","shell.execute_reply.started":"2023-10-15T09:54:21.103502Z","shell.execute_reply":"2023-10-15T09:54:21.107152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a pipeline that applies the 'preprocessor' (ColumnTransformer) to the data.\n# This pipeline can be extended with other steps such as a machine learning model in the future if needed.\npipeline = Pipeline(steps=[(\"preprocessor\", preprocessor)])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.164238Z","iopub.execute_input":"2023-10-15T09:54:21.165149Z","iopub.status.idle":"2023-10-15T09:54:21.169242Z","shell.execute_reply.started":"2023-10-15T09:54:21.165098Z","shell.execute_reply":"2023-10-15T09:54:21.168218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.173010Z","iopub.execute_input":"2023-10-15T09:54:21.173330Z","iopub.status.idle":"2023-10-15T09:54:21.256932Z","shell.execute_reply.started":"2023-10-15T09:54:21.173303Z","shell.execute_reply":"2023-10-15T09:54:21.255928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.258487Z","iopub.execute_input":"2023-10-15T09:54:21.258753Z","iopub.status.idle":"2023-10-15T09:54:21.280401Z","shell.execute_reply.started":"2023-10-15T09:54:21.258729Z","shell.execute_reply":"2023-10-15T09:54:21.278991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.281830Z","iopub.execute_input":"2023-10-15T09:54:21.282798Z","iopub.status.idle":"2023-10-15T09:54:21.290205Z","shell.execute_reply.started":"2023-10-15T09:54:21.282758Z","shell.execute_reply":"2023-10-15T09:54:21.288711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.292009Z","iopub.execute_input":"2023-10-15T09:54:21.292557Z","iopub.status.idle":"2023-10-15T09:54:21.302356Z","shell.execute_reply.started":"2023-10-15T09:54:21.292530Z","shell.execute_reply":"2023-10-15T09:54:21.301413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the 'Transported' column as the target variable 'y' and converting it to integer type\ny = df[\"Transported\"].astype(int)\n\n# Removing the 'Transported' column from the feature matrix 'X' as it's the target variable\nX = X.drop([\"Transported\"], axis=1)\n\n# Applying the previously defined preprocessing pipeline to the feature matrix 'X'\n# This will impute missing values, scale numerical features, and one-hot encode categorical features.\nX_preprocessed = pipeline.fit_transform(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.303730Z","iopub.execute_input":"2023-10-15T09:54:21.303977Z","iopub.status.idle":"2023-10-15T09:54:21.378973Z","shell.execute_reply.started":"2023-10-15T09:54:21.303955Z","shell.execute_reply":"2023-10-15T09:54:21.378115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_preprocessed.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.380463Z","iopub.execute_input":"2023-10-15T09:54:21.380736Z","iopub.status.idle":"2023-10-15T09:54:21.385881Z","shell.execute_reply.started":"2023-10-15T09:54:21.380712Z","shell.execute_reply":"2023-10-15T09:54:21.384902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_preprocessed","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.423981Z","iopub.execute_input":"2023-10-15T09:54:21.424330Z","iopub.status.idle":"2023-10-15T09:54:21.431737Z","shell.execute_reply.started":"2023-10-15T09:54:21.424303Z","shell.execute_reply":"2023-10-15T09:54:21.430733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing machine learning classifiers/algorithms\nfrom sklearn.linear_model import LogisticRegression  # Logistic Regression model\nfrom sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\nfrom sklearn.svm import SVC  # Support Vector Machine classifier\nfrom sklearn.ensemble import RandomForestClassifier  # Random Forest classifier\nfrom sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting classifier\n\n# Importing tools for model selection and evaluation\nfrom sklearn.model_selection import GridSearchCV  # Tool for hyperparameter tuning using cross-validation\nfrom sklearn.model_selection import KFold  # Provides train/test indices to split data into train/test sets\nfrom sklearn.model_selection import cross_val_score  # Evaluate a score by cross-validation\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.461436Z","iopub.execute_input":"2023-10-15T09:54:21.462044Z","iopub.status.idle":"2023-10-15T09:54:21.467899Z","shell.execute_reply.started":"2023-10-15T09:54:21.462004Z","shell.execute_reply":"2023-10-15T09:54:21.466913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the function to split datasets\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the preprocessed data into training and testing sets\n# 80% of the data will be used for training and 20% for testing\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.507665Z","iopub.execute_input":"2023-10-15T09:54:21.508304Z","iopub.status.idle":"2023-10-15T09:54:21.516156Z","shell.execute_reply.started":"2023-10-15T09:54:21.508263Z","shell.execute_reply":"2023-10-15T09:54:21.515203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a dictionary of classifiers/algorithms.\n# This dictionary can be easily extended to add more models or be iterated over for model evaluation.\nmodels = {\n    \"RandomForest\": RandomForestClassifier(),  # Random Forest classifier\n    \"SupportVectorMachine\": SVC(),  # Support Vector Machine classifier\n    \"XGBoost\": GradientBoostingClassifier()  # Gradient Boosting classifier (commonly referred to as XGBoost, though XGBoost is technically a separate library)\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.615094Z","iopub.execute_input":"2023-10-15T09:54:21.615793Z","iopub.status.idle":"2023-10-15T09:54:21.621926Z","shell.execute_reply.started":"2023-10-15T09:54:21.615752Z","shell.execute_reply":"2023-10-15T09:54:21.620504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a dictionary of hyperparameter grids for each classifier\nparam_grids = {\n    \"RandomForest\": {\n        \"n_estimators\": [100, 150, 200],\n        \"criterion\": [\"gini\", \"entropy\", \"logloss\"]\n    },\n    \"SupportVectorMachine\": {\n        \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n        \"gamma\": [\"scale\", \"auto\"],\n        \"C\": [0.5, 1.0, 5.0, 10.0]\n    },\n    \"XGBoost\": {\n        \"loss\": [\"log_loss\", \"exponential\"],\n        \"learning_rate\": [0.1, 0.01, 0.05],\n        \"n_estimators\": [200, 250, 300]\n    }\n}\n\n# Initializing a KFold cross-validator with 3 splits\ncv = KFold(n_splits=3, shuffle=True)\n\n# Dictionary to store the GridSearchCV objects for each model\ngrids = {}\n\n# Looping over the defined classifiers to perform hyperparameter tuning\nfor model_name, model in models.items():\n    # Applying GridSearchCV for hyperparameter tuning\n    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n    grids[model_name].fit(X_train, y_train)\n    \n    # Extracting the best parameters and best RMSE for each model\n    best_params = grids[model_name].best_params_\n    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n    # Printing the results\n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:54:21.623844Z","iopub.execute_input":"2023-10-15T09:54:21.624265Z","iopub.status.idle":"2023-10-15T09:56:46.889345Z","shell.execute_reply.started":"2023-10-15T09:54:21.624235Z","shell.execute_reply":"2023-10-15T09:56:46.887909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the mean_squared_error metric to evaluate the model's performance\nfrom sklearn.metrics import mean_squared_error\n\n# Looping over the trained models to compute and print the RMSE on the test set\nfor model_name in grids.keys():\n    # Predicting the target values using the current model\n    predictions = grids[model_name].predict(X_test)\n    \n    # Calculating the RMSE for the current model\n    rmse = np.sqrt(mean_squared_error(predictions, y_test))\n    \n    # Printing the RMSE for the current model\n    print(f'{model_name}: {rmse}')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:46.890979Z","iopub.execute_input":"2023-10-15T09:56:46.891525Z","iopub.status.idle":"2023-10-15T09:56:47.462190Z","shell.execute_reply.started":"2023-10-15T09:56:46.891452Z","shell.execute_reply":"2023-10-15T09:56:47.461283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the test dataset from the specified path into a pandas DataFrame 'df_test'\ndf_test = pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.464097Z","iopub.execute_input":"2023-10-15T09:56:47.464410Z","iopub.status.idle":"2023-10-15T09:56:47.486310Z","shell.execute_reply.started":"2023-10-15T09:56:47.464383Z","shell.execute_reply":"2023-10-15T09:56:47.485476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.487513Z","iopub.execute_input":"2023-10-15T09:56:47.487835Z","iopub.status.idle":"2023-10-15T09:56:47.511023Z","shell.execute_reply.started":"2023-10-15T09:56:47.487810Z","shell.execute_reply":"2023-10-15T09:56:47.509870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating separate DataFrames to store predictions of different models.\n# Each DataFrame is initialized with the \"PassengerId\" column from the test set.\n\n# DataFrame for Logistic Regression model predictions\nLogistic_df = df_test[[\"PassengerId\"]].copy()\n\n# DataFrame for Support Vector Machine model predictions\nSupportVectorMachine_df = df_test[[\"PassengerId\"]].copy()\n\n# DataFrame for XGBoost model predictions\nXGBoost_df = df_test[[\"PassengerId\"]].copy()\n\n# DataFrame for Random Forest model predictions\nRandomForest_df = df_test[[\"PassengerId\"]].copy()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.512141Z","iopub.execute_input":"2023-10-15T09:56:47.512411Z","iopub.status.idle":"2023-10-15T09:56:47.521318Z","shell.execute_reply.started":"2023-10-15T09:56:47.512388Z","shell.execute_reply":"2023-10-15T09:56:47.519922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the preprocessing function to the 'df_test' DataFrame to prepare it for predictions\ndf_test = preprocessing(df_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.522390Z","iopub.execute_input":"2023-10-15T09:56:47.522675Z","iopub.status.idle":"2023-10-15T09:56:47.574386Z","shell.execute_reply.started":"2023-10-15T09:56:47.522651Z","shell.execute_reply":"2023-10-15T09:56:47.573596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.575339Z","iopub.execute_input":"2023-10-15T09:56:47.576221Z","iopub.status.idle":"2023-10-15T09:56:47.599730Z","shell.execute_reply.started":"2023-10-15T09:56:47.576192Z","shell.execute_reply":"2023-10-15T09:56:47.598671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the previously defined preprocessing pipeline to the 'df_test' DataFrame \n# This will impute missing values, scale numerical features, and one-hot encode categorical features.\ndf_test_preprocessed = pipeline.transform(df_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.603700Z","iopub.execute_input":"2023-10-15T09:56:47.604004Z","iopub.status.idle":"2023-10-15T09:56:47.628352Z","shell.execute_reply.started":"2023-10-15T09:56:47.603979Z","shell.execute_reply":"2023-10-15T09:56:47.627380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the RandomForest model (previously tuned with GridSearchCV) to make predictions on the preprocessed test set\ny_RandomForest = grids['RandomForest'].predict(df_test_preprocessed)\n\n# Converting the predicted values to boolean type (True/False)\ny_RandomForest = y_RandomForest.astype(bool)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.629464Z","iopub.execute_input":"2023-10-15T09:56:47.629811Z","iopub.status.idle":"2023-10-15T09:56:47.807270Z","shell.execute_reply.started":"2023-10-15T09:56:47.629786Z","shell.execute_reply":"2023-10-15T09:56:47.806343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the XGBoost model (previously tuned with GridSearchCV) to make predictions on the preprocessed test set\ny_XGBoostRegression = grids['XGBoost'].predict(df_test_preprocessed)\n\n# Converting the predicted values to boolean type (True/False)\ny_XGBoostRegression = y_XGBoostRegression.astype(bool)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.808536Z","iopub.execute_input":"2023-10-15T09:56:47.809317Z","iopub.status.idle":"2023-10-15T09:56:47.830915Z","shell.execute_reply.started":"2023-10-15T09:56:47.809279Z","shell.execute_reply":"2023-10-15T09:56:47.829962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the Support Vector Machine model (previously tuned with GridSearchCV) to make predictions on the preprocessed test set\ny_SupportVectorMachine = grids[\"SupportVectorMachine\"].predict(df_test_preprocessed)\n\n# Converting the predicted values to boolean type (True/False)\ny_SupportVectorMachine = y_SupportVectorMachine.astype(bool)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:47.832231Z","iopub.execute_input":"2023-10-15T09:56:47.832509Z","iopub.status.idle":"2023-10-15T09:56:48.968977Z","shell.execute_reply.started":"2023-10-15T09:56:47.832485Z","shell.execute_reply":"2023-10-15T09:56:48.968034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the predictions from the RandomForest model to the 'Transported' column of the 'RandomForest_df' DataFrame\nRandomForest_df[\"Transported\"] = y_RandomForest\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:48.970493Z","iopub.execute_input":"2023-10-15T09:56:48.970921Z","iopub.status.idle":"2023-10-15T09:56:48.976213Z","shell.execute_reply.started":"2023-10-15T09:56:48.970891Z","shell.execute_reply":"2023-10-15T09:56:48.975106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the predictions from the Support Vector Machine model to the 'Transported' column of the 'SupportVectorMachine_df' DataFrame\nSupportVectorMachine_df[\"Transported\"] = y_SupportVectorMachine\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:48.977791Z","iopub.execute_input":"2023-10-15T09:56:48.978148Z","iopub.status.idle":"2023-10-15T09:56:48.990504Z","shell.execute_reply.started":"2023-10-15T09:56:48.978096Z","shell.execute_reply":"2023-10-15T09:56:48.989418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assigning the predictions from the XGBoost model to the 'Transported' column of the 'XGBoost_df' DataFrame\nXGBoost_df[\"Transported\"] = y_XGBoostRegression\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:48.992238Z","iopub.execute_input":"2023-10-15T09:56:48.992681Z","iopub.status.idle":"2023-10-15T09:56:49.003356Z","shell.execute_reply.started":"2023-10-15T09:56:48.992655Z","shell.execute_reply":"2023-10-15T09:56:49.002328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SupportVectorMachine_df","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:49.004649Z","iopub.execute_input":"2023-10-15T09:56:49.005261Z","iopub.status.idle":"2023-10-15T09:56:49.026869Z","shell.execute_reply.started":"2023-10-15T09:56:49.005229Z","shell.execute_reply":"2023-10-15T09:56:49.025778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBoost_df","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:49.028289Z","iopub.execute_input":"2023-10-15T09:56:49.029150Z","iopub.status.idle":"2023-10-15T09:56:49.039429Z","shell.execute_reply.started":"2023-10-15T09:56:49.029096Z","shell.execute_reply":"2023-10-15T09:56:49.038413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SupportVectorMachine_df.to_csv(\"submission_support_vector_machine.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:49.040627Z","iopub.execute_input":"2023-10-15T09:56:49.041340Z","iopub.status.idle":"2023-10-15T09:56:49.052813Z","shell.execute_reply.started":"2023-10-15T09:56:49.041309Z","shell.execute_reply":"2023-10-15T09:56:49.052080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBoost_df.to_csv(\"submission_XGBoost.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:49.054359Z","iopub.execute_input":"2023-10-15T09:56:49.055306Z","iopub.status.idle":"2023-10-15T09:56:49.068426Z","shell.execute_reply.started":"2023-10-15T09:56:49.055267Z","shell.execute_reply":"2023-10-15T09:56:49.067450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RandomForest_df.to_csv(\"submission_RandomForest.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T09:56:49.069636Z","iopub.execute_input":"2023-10-15T09:56:49.070008Z","iopub.status.idle":"2023-10-15T09:56:49.081282Z","shell.execute_reply.started":"2023-10-15T09:56:49.069971Z","shell.execute_reply":"2023-10-15T09:56:49.080155Z"},"trusted":true},"execution_count":null,"outputs":[]}]}